# Spoken-Human-Robot-Interaction-Project

# INTRODUCTION 
Human Robot Interaction (HRI) is the field of study dedicated to understanding, designing, and evaluating robot systems for use   by or with humans.1 
  The number of progress made in the last years in different fields like engineering, cognitive sciences, communications  and many more, increased inevitably the desire to develop more complex interactions between humans and robot systems. The most natural way to implement interactions is through the use of spoken language. The advantage is that everyone is able to hold a conversation. But the downsides are many more, noisy inputs, ambiguity of the languages, but also the context in which the interaction takes place. Yet no another known method can outclass the easiness of natural language. 
  A task-oriented sds is an intelligent agent that can help to complete or achieve certain tasks through the use of spoken interactions. This devices started to be quite common in the last few years thanks to the large amount of data that the big industries, Google and Amazon, have been able to gather. An interaction with an sds is very simple, the communication is divided in turns, the user asks something to the agent who will try to interpret the command received and answer consequently. The intrepretation of a command can be divided in different phases. Each of this phases is designed to extract different information and feature from the command such that it will be possible to give to it a meaning. This phases are: 
- Automatic Speech Recognition 
- Morpho-syntactic analysis 
- Semantic analysis 
 
The particular type of task-oriented sds that we realized is an agent that plays as a waiter in a restaurant. It has to be able to interact with customers by asking for their orders, request confirmations and keep track of everything that has been ordered. 

# AUTOMATIC SPEECH RECOGNITION 
The automatic speech recognition (ASR) is that process that allows to generates text starting from an audio signal. The are lots of different problems that the system must deal with: 
- Audio signals are noisy. 
- Spoken language is very different from written language, homophones, merged sounds and missing space are all issues that can lead to a wrong recognition of the signal. 
  The tool used in this project for the ASR phase is PyAudio, the speech recognition library for Python, which takes in input the audio signal and return an object that can be used for further operations. 

# MORPHO-SYNTACTIC ANALYSIS 
This second phase can be divided into three more sub-phases. The first is the morphologic analysis in which it has to be verified the morphological correctness of the words, that is remove any unwanted character, tokenize the text and obtain lemmas. The tokenization simply divide the sentences into tokens which correspond exactly to the single words of a sentence. The lemmatization is instead more complicated, here we try to determine if a word has the same root of another word. This can be usefull for verbs that are not in the basic form, or for words that are in a plural forms. 
  The second sub-phase is the POS-Tagging process, where to each token of the sentence it is associated a POS-tag. This procedure specifies the grammatical meaning of the tokens in the phrase, which might be different from situation to situation. By detecting whether a word is a noun or a verb allows to extract informations for the the consecutive sub-phase. 
  The third sub-phase is the syntactic analysis in which we use all the information gathered so far in order to check if a sentence if syntactically correct. Syntax is based on constituents, single or group of words that compose particular syntactic structure (NP, VP, S, …). 
  Constituents, POS-tags and words can be used to represent the syntactic tree, a particular graphical representation of the syntactic structure of a sentence that can be parsed in order to decide if the sentence in correct. Nowadays the most modern parsers use a dependency-based representation to navigate the syntactic trees. The fundamental concept is that all the words of a sentence are connected by at least one dependency relation. A dependency relation is composed by a label that define the nature of the dependency relation, the head, the word that “govern” the relation and the dependent. 
  The tool used to implement all this three sub-phases is the StanfordCoreNLP – Natural Language Software. It has been used to reconstruct the command than can be given to the to the agent. 

# SEMANTIC ANALYSIS 
In this last phase we study the meaning of the sentence, such that the agent will be able to understand it and provide a correct answer. The study of the semantic can be done from two points of view. The lexical semantics studies the meaning of the words, while the sentence semantics studies the meaning of the entire sentence. 
  The way syntactic analysis has been implemented is very simple. The agent has a vocabulary of words that it knows and that it should be able to recognize. Through the use of all the tools mentioned so far it will be possible to determine the meaning of the sentence. In order to make everything more accurate, the agent will always requires a confirmation from the user. 

# IMPLEMENTATION 
The very first thing that we did was to parse the result obtained by the Stanford CoreNLP parser class.  This class offers a particular method called annotate() that take as input the following arguments: 
- the text that has to be analyzed 
- the format of the output of the analyzed text 
- different annotators, that specify what are the properties of the text that should be extracted (sentiment analysis, pos tagging, tokenization, dependency relations, …). 

The informations that we decided to acquire from it are the tokens and the lemmas contained in the phrase, the POS-tags of each token and the dependency relations among all of them. Then we passed to analyze the phrase in such way that it was possible to extract from it the root element, the main nominal subject and the main object of the phrase, but also the principal verbs. Concerning the verbs we made the following assumpion, in spoken language it is very likely to use phrases that are not grammatically correct, particularly in all those contexts that do not require a formal language. So for example clauses like ‘A table for two’, ‘A slice of pizza’,  are perfectly normal for spoken interactions; thus whenever we were presented with this situation, we infered that we could handle the meaning of the phrase as an order, so as a request made by the user. The most arduous part was trying to list all the peculiarity and possible situations that can occur for this type of interactions. 
  The next task we managed was to identify the basic elements of a clause. In english any phrase is composed always by at least three elements, a subject, a verb and an object to which the verb is referred. All this information can be found by exploiting the dependency relations and the POS-tags of the tokens. 
  One of the dependency relations that the Stanford parser is able to generate is called ‘root’, which is applied to the main element of the phrase. Thus, by simply obtaining the root element we can obtain most of the informations. The root element appear to be every time either the main verb or the subject of the phrase. Now, if the root is a verb then finding the subject is trivial because a verb without a personal pronoun is either a command (and so we treat it as previously said) or the clause is missing something. With this observation, we simply have to look for a distinct depency relation called ‘nsubj’, which contain the root verb and the personal pronoun that we are looking for. If instead the root is the subject we can apply the exact opposite reasoning.  Furthermore the verbs can efficiently be found by exploiting the POS-tags property. Every verb is associated with the tag with value ‘VB’, so the idea is to first find them, and then check whether one of the verbs found appear as a root element or has an element of the ‘nsubj’ dependency relation. 
  Regarding the object of the phrase, the situation start to be tricking. In english grammar an object is tipically a noun phrase (a noun or a pronoun followed by other words) and it generally follows the verb of the clause. From this we can conclude that either we try to search for a noun phrase, or instead we can consider all the nouns that appear after a verb. We choosed to follow the former approach because the output from the Stanford parser give us the option to do the syntactic analysis aswell, where it highlight the consistuents of a phrase. 
  With a subject, a verb and an object, defining the general meaning of the phrase is direct. By  defining the vocabulary of the words that the agent should know, and by encoding some general sentences that the agent has to pronounce depending on the meaning of the phrase, we can obtain discrete results. The particular library that allow our agent to speak is the Google Text-To-Speech (gTTS) library which is very basic but works properly. 
